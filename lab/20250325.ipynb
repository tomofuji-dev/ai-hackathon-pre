{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02233591-9b6a-4be7-a717-33b1d687d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/r-f/wav2vec-english-speech-emotion-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a885a3d-2c93-4211-9c90-2c13153a25dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/t.fuji/.cache/huggingface/hub/models--firdhokk--speech-emotion-recognition-with-openai-whisper-large-v3/snapshots/83e7cc6cebb3978e4cc314ebad9f1614c177a94a/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"apply_spec_augment\": false,\n",
      "  \"architectures\": [\n",
      "    \"WhisperForAudioClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"d_model\": 1280,\n",
      "  \"decoder_attention_heads\": 20,\n",
      "  \"decoder_ffn_dim\": 5120,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 32,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 20,\n",
      "  \"encoder_ffn_dim\": 5120,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 32,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"angry\",\n",
      "    \"1\": \"disgust\",\n",
      "    \"2\": \"fearful\",\n",
      "    \"3\": \"happy\",\n",
      "    \"4\": \"neutral\",\n",
      "    \"5\": \"sad\",\n",
      "    \"6\": \"surprised\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"angry\": 0,\n",
      "    \"disgust\": 1,\n",
      "    \"fearful\": 2,\n",
      "    \"happy\": 3,\n",
      "    \"neutral\": 4,\n",
      "    \"sad\": 5,\n",
      "    \"surprised\": 6\n",
      "  },\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"median_filter_width\": 7,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_mel_bins\": 128,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"scale_embedding\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 51866\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/t.fuji/.cache/huggingface/hub/models--firdhokk--speech-emotion-recognition-with-openai-whisper-large-v3/snapshots/83e7cc6cebb3978e4cc314ebad9f1614c177a94a/model.safetensors\n",
      "All model checkpoint weights were used when initializing WhisperForAudioClassification.\n",
      "\n",
      "All the weights of WhisperForAudioClassification were initialized from the model checkpoint at firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForAudioClassification for predictions without further training.\n",
      "loading configuration file preprocessor_config.json from cache at /Users/t.fuji/.cache/huggingface/hub/models--firdhokk--speech-emotion-recognition-with-openai-whisper-large-v3/snapshots/83e7cc6cebb3978e4cc314ebad9f1614c177a94a/preprocessor_config.json\n",
      "Feature extractor WhisperFeatureExtractor {\n",
      "  \"chunk_length\": 30,\n",
      "  \"dither\": 0.0,\n",
      "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "  \"feature_size\": 128,\n",
      "  \"hop_length\": 160,\n",
      "  \"n_fft\": 400,\n",
      "  \"n_samples\": 480000,\n",
      "  \"nb_max_frames\": 3000,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0.0,\n",
      "  \"processor_class\": \"WhisperProcessor\",\n",
      "  \"return_attention_mask\": false,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model_id = \"firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3\"\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_id)\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id, do_normalize=True)\n",
    "id2label = model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c91e701f-bbff-469d-934c-0e09ab1e7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_path, feature_extractor, max_duration=30.0):\n",
    "    audio_array, sampling_rate = librosa.load(audio_path, sr=feature_extractor.sampling_rate)\n",
    "    \n",
    "    max_length = int(feature_extractor.sampling_rate * max_duration)\n",
    "    if len(audio_array) > max_length:\n",
    "        audio_array = audio_array[:max_length]\n",
    "    else:\n",
    "        audio_array = np.pad(audio_array, (0, max_length - len(audio_array)))\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        audio_array,\n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "211af7ff-90eb-4076-bed3-a885ec9b0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(audio_path, model, feature_extractor, id2label, max_duration=30.0):\n",
    "    inputs = preprocess_audio(audio_path, feature_extractor, max_duration)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "    predicted_label = id2label[predicted_id]\n",
    "    \n",
    "    return logits[0][predicted_id], predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15d60149-c07e-4e07-b115-72daac0b0a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion: (tensor(4.7272), 'happy')\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"./test.mp3\"\n",
    "\n",
    "predicted_emotion = predict_emotion(audio_path, model, feature_extractor, id2label)\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83b1bf0d-0468-4c47-8f13-fca5b00fe33a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'angry',\n",
       " 1: 'disgust',\n",
       " 2: 'fearful',\n",
       " 3: 'happy',\n",
       " 4: 'neutral',\n",
       " 5: 'sad',\n",
       " 6: 'surprised'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77251fa6-6b8c-40d1-8006-81923690ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_id = torch.argmax(logits, dim=-1).item()\n",
    "predicted_label = id2label[predicted_id]    \n",
    "return predicted_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
